{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting input sentences into numeric sequences. Similar to images in CNNs, we need to prepare text data with uniform size before feeding it to our model. We will see how to do these in the next sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab, you saw how to use the TextVectorization layer to build a vocabulary from your corpus. It generates a list where more frequent words have lower indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 14:42:50.162812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-24 14:42:51.139928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '[UNK]', 'my', 'love', 'you', 'i', 'dog', 'cat', 'do']\n",
      "\n",
      "with indices:\n",
      "\n",
      "0 \n",
      "1 [UNK]\n",
      "2 my\n",
      "3 love\n",
      "4 you\n",
      "5 i\n",
      "6 dog\n",
      "7 cat\n",
      "8 do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 14:42:52.949187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-24 14:42:52.972362: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog.\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "    \"Do you love my cat?\"\n",
    "]\n",
    "\n",
    "# Initialize the layer\n",
    "vec_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Compute the vocab\n",
    "vec_layer.adapt(sentences)\n",
    "\n",
    "# get the vocab\n",
    "vocab_tf = vec_layer.get_vocabulary()\n",
    "\n",
    "print(f\"Vocabulary: {vocab_tf}\")\n",
    "print(\"\\nwith indices:\\n\")\n",
    "for index, word in enumerate(vocab_tf):\n",
    "    print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['<pad>', '<unk>', 'love', 'my', 'cat', 'dog', 'i', 'you', '!', '.', '?', 'do']\n",
      "\n",
      "With Indices:\n",
      "\n",
      "{'?': 10, '!': 8, '.': 9, 'i': 6, 'dog': 5, 'do': 11, 'cat': 4, 'my': 3, 'you': 7, 'love': 2, '<unk>': 1, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog.\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "    \"Do you love my cat?\"\n",
    "]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_token(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_token(sentences), specials=['<pad>', '<unk>'])\n",
    "\n",
    "print(f\"Vocabulary: {vocab.get_itos()}\")\n",
    "print(\"\\nWith Indices:\\n\")\n",
    "print(vocab.get_stoi())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the result to convert each of the input sentences into integer sequences. See how that's done below given a single input string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5 3 2 6], shape=(4,), dtype=int64)\n",
      "vocab: ['', '[UNK]', 'my', 'love', 'you', 'i', 'dog', 'cat', 'do']\n"
     ]
    }
   ],
   "source": [
    "# string input\n",
    "sample_input = \"I love my dog\"\n",
    "\n",
    "# convert string input to integer sequence\n",
    "seq = vec_layer(sample_input)\n",
    "\n",
    "print(seq)\n",
    "\n",
    "# To Check\n",
    "print(f\"vocab: {vocab_tf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 3, 5]\n",
      "{'?': 10, '!': 8, '.': 9, 'i': 6, 'dog': 5, 'do': 11, 'cat': 4, 'my': 3, 'you': 7, 'love': 2, '<unk>': 1, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "# string input\n",
    "sample_input = \"I love my dog\"\n",
    "\n",
    "# convert string input to integer sequence\n",
    "tokens = tokenizer(sample_input)\n",
    "\n",
    "# get the tokens\n",
    "seq = vocab(tokens)\n",
    "\n",
    "print(seq)\n",
    "\n",
    "# to check\n",
    "print(vocab.get_stoi())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, you simply pass in the string to the layer which already learned the vocabulary, and it will output the integer sequence as a tf.Tensor. In this case, the result is [6 3 2 4]. You can look at the token index printed above to verify that it matches the indices for each word in the input string.\n",
    "\n",
    "For a given list of string inputs (such as the 4-item sentences list above), you will need to apply the layer to each input. There's more than one way to do this. Let's first use the map() method and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "I love my dog. --> [5 3 2 6]\n",
      "I love my cat --> [5 3 2 7]\n",
      "You love my dog! --> [4 3 2 6]\n",
      "Do you love my cat? --> [8 4 3 2 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 14:42:55.523119: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [4]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)\n",
    "\n",
    "\n",
    "# convert sentences to tf data\n",
    "sentences_dataset = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "\n",
    "# define a mapping function to convert each sample input\n",
    "sequences = sentences_dataset.map(vec_layer)\n",
    "\n",
    "# print integer sequences\n",
    "\n",
    "for sentence, sequence in zip(sentences, sequences):\n",
    "    print(f\"{sentence} --> {sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "I love my dog. --> [6, 2, 3, 5, 9]\n",
      "I love my cat --> [6, 2, 3, 4]\n",
      "You love my dog! --> [7, 2, 3, 5, 8]\n",
      "Do you love my cat? --> [11, 7, 2, 3, 4, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'?': 10,\n",
       " '!': 8,\n",
       " '.': 9,\n",
       " 'i': 6,\n",
       " 'dog': 5,\n",
       " 'do': 11,\n",
       " 'cat': 4,\n",
       " 'my': 3,\n",
       " 'you': 7,\n",
       " 'love': 2,\n",
       " '<unk>': 1,\n",
       " '<pad>': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences)\n",
    "\n",
    "# get tokens for sentences\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    # print(tokens)\n",
    "    seq = vocab(tokens)\n",
    "    print(f\"{sentence} --> {seq}\")\n",
    "\n",
    "# To check\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of varying lengths to have a uniform size by padding or truncating tokens from the sequences. Padding is more common to preserve information.\n",
    "\n",
    "Recall that your vocabulary reserves a special token index 0 for padding. It will add that token (called post padding) if you pass in a list of string inputs to the layer. See an example below. Notice that you have the same output as above but the integer sequences are already post-padded with 0 up to the length of the longest sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "INPUT:\n",
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "\n",
      "OUTPUT:\n",
      "tf.Tensor(\n",
      "[[5 3 2 6 0]\n",
      " [5 3 2 7 0]\n",
      " [4 3 2 6 0]\n",
      " [8 4 3 2 7]], shape=(4, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(sentences)\n",
    "\n",
    "# apply the layer to the input list\n",
    "seq_post = vec_layer(sentences)\n",
    "print('INPUT:')\n",
    "print(sentences)\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(seq_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want pre-padding, you can use the pad_sequences() utility to prepend a padding token to the sequences. Notice that the padding argument is set to pre. This is just for clarity. The function already has this set as the default so you can opt to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None)>\n",
      "INPUT:\n",
      "[5 3 2 6]\n",
      "[5 3 2 7]\n",
      "[4 3 2 6]\n",
      "[8 4 3 2 7]\n",
      "\n",
      "OUTPUT:\n",
      "[[0 5 3 2 6]\n",
      " [0 5 3 2 7]\n",
      " [0 4 3 2 6]\n",
      " [8 4 3 2 7]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "\n",
    "seq_pre = tf.keras.utils.pad_sequences(sequences, padding='pre')\n",
    "\n",
    "# # For Post\n",
    "# seq_post = tf.keras.utils.pad_sequences(sequences, padding='post')\n",
    "# seq_post = tf.keras.utils.pad_sequences(sequences)\n",
    "\n",
    "\n",
    "print('INPUT:')\n",
    "[print(sequence.numpy()) for sequence in sequences]\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(seq_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "[5 3 2 6]\n",
      "[5 3 2 7]\n",
      "[4 3 2 6]\n",
      "[8 4 3 2 7]\n",
      "\n",
      "OUTPUT:\n",
      "[[5 3 2 6]\n",
      " [5 3 2 7]\n",
      " [4 3 2 6]\n",
      " [4 3 2 7]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "\n",
    "seq_pre = tf.keras.utils.pad_sequences(sequences, maxlen=4, padding='pre')\n",
    "\n",
    "print('INPUT:')\n",
    "[print(sequence.numpy()) for sequence in sequences]\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(seq_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None)>\n",
      "INPUT:\n",
      "[5 3 2 6]\n",
      "[5 3 2 7]\n",
      "[4 3 2 6]\n",
      "[8 4 3 2 7]\n",
      "\n",
      "OUTPUT:\n",
      "[[5 3 2 6]\n",
      " [5 3 2 7]\n",
      " [4 3 2 6]\n",
      " [8 4 3 2]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "\n",
    "seq_post = tf.keras.utils.pad_sequences(sequences, maxlen=4, truncating='post', padding='pre')\n",
    "\n",
    "print('INPUT:')\n",
    "[print(sequence.numpy()) for sequence in sequences]\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(seq_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to prepare your sequences for prepadding is to set the TextVectorization to output a ragged tensor. This means the output will not be automatically post-padded. See the output sequences here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "<tf.RaggedTensor [[5, 3, 2, 6], [5, 3, 2, 7], [4, 3, 2, 6], [8, 4, 3, 2, 7]]>\n"
     ]
    }
   ],
   "source": [
    "print(sentences)\n",
    "# Set the layer to output a ragged tensor\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(ragged=True)\n",
    "\n",
    "# Compute the vocabulary\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# Apply the layer to the sentences\n",
    "ragged_sequences = vectorize_layer(sentences)\n",
    "\n",
    "# Print the results\n",
    "print(ragged_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, you can now pass it directly to the pad_sequences() utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 5 3 2 6]\n",
      " [0 5 3 2 7]\n",
      " [0 4 3 2 6]\n",
      " [8 4 3 2 7]]\n"
     ]
    }
   ],
   "source": [
    "# Pre-pad the sequences in the ragged tensor\n",
    "sequences_pre = tf.keras.utils.pad_sequences(ragged_sequences.numpy())\n",
    "\n",
    "# Print the results\n",
    "print(sequences_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "tensor_list: \n",
      "\n",
      "[tensor([ 4,  2,  3, 10]), tensor([4, 2, 3, 6]), tensor([5, 2, 3, 9]), tensor([8, 5, 2, 3, 7])]\n",
      "tensor([[ 4,  2,  3, 10,  0],\n",
      "        [ 4,  2,  3,  6,  0],\n",
      "        [ 5,  2,  3,  9,  0],\n",
      "        [ 8,  5,  2,  3,  7]])\n",
      "\n",
      "{'dog.': 10, 'dog!': 9, 'cat?': 7, 'do': 8, 'cat': 6, 'i': 4, 'my': 3, 'you': 5, 'love': 2, '<unk>': 1, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "print(sentences)\n",
    "\n",
    "tokenizer = lambda x: x.lower().split()\n",
    "\n",
    "def yield_token(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_token(sentences), specials=['<pad>', '<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "#numericalized\n",
    "list_num = []\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    seq = vocab(tokens)\n",
    "    list_num.append(torch.tensor(seq))\n",
    " \n",
    "print(\"tensor_list: \\n\")\n",
    "print(list_num)\n",
    "\n",
    "padding = pad_sequence(sequences=list_num, batch_first=True, padding_value=vocab['<pad>'])\n",
    "print(padding)\n",
    "print()\n",
    "print(vocab.get_stoi())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch build in only supports post-padding. To pre-pad define function manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  2,  3, 10],\n",
       "        [ 0,  4,  2,  3,  6],\n",
       "        [ 0,  5,  2,  3,  9],\n",
       "        [ 8,  5,  2,  3,  7]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def pre_pad(sequences, padding_value, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        pad_len = max_len - len(seq)\n",
    "        padded_seq = torch.cat([torch.full((pad_len,), padding_value), seq])\n",
    "        padded.append(padded_seq)\n",
    "    return torch.stack(padded)\n",
    "\n",
    "\n",
    "pad_pre = pre_pad(list_num, padding_value=vocab['<pad>'])\n",
    "pad_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncating is also not supported in PyTorch built-in. So we will have to define it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing_extensions import Literal\n",
    "literal_ = Literal['pre', 'post']\n",
    "# print(list_num)\n",
    "\n",
    "def truncate_torch(sequences: list[torch.Tensor], len_: int, type: literal_):\n",
    "\n",
    "    list_len = [len(seq) for seq in sequences]\n",
    "    max_len = max(list_len)\n",
    "    min_len = min(list_len)\n",
    "    new_seq_list = []\n",
    "    if len_ < max_len:\n",
    "        for seq in sequences:\n",
    "            len_to_del = len(seq) - len_\n",
    "            if type == 'pre':\n",
    "                # indices_to_del = torch.Tensor([i for i in range(0, len_to_del)])\n",
    "                new_seq = seq[len_to_del:]\n",
    "            elif type == 'post':\n",
    "                # indices_to_del = torch.Tensor([i for i in range(-1, -len_to_del-1, -1)])\n",
    "                new_seq = seq[0:-len_to_del]\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Please choose type from {literal_}\")\n",
    "            new_seq_list.append(new_seq)\n",
    "            # print(new_seq_list)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Nothing to truncate. Given length to truncate - {len_} > maximum length of sequences - {max_len}\")\n",
    "    \n",
    "    return new_seq_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 2,  3, 10]), tensor([2, 3, 6]), tensor([2, 3, 9]), tensor([2, 3, 7])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_torch(list_num, 3, 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([4, 2, 3]), tensor([4, 2, 3]), tensor([5, 2, 3]), tensor([8, 5, 2])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_torch(list_num, 3, 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write functions like this for combine truncating and padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Out-of-vocabulary tokens\n",
    "\n",
    "Lastly, you'll see what the other special token is for. The layer will use the token index 1 when you have input words that are not found in the vocabulary list. For example, you may decide to collect more text after your initial training and decide to not recompute the vocabulary. You will see this in action in the cell below. Notice that the token 1 is inserted for words that are not found in the list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really love my dog --> [5 1 3 2 6]\n",
      "my dogs love my manatee --> [2 1 3 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'my', 'love', 'you', 'i', 'dog', 'cat', 'do']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_oov = [\n",
    "    \"i really love my dog\",\n",
    "    \"my dogs love my manatee\"\n",
    "]\n",
    "\n",
    "seq_with_oov = vectorize_layer(sentences_oov)\n",
    "\n",
    "for sentence, sequence in zip(sentences_oov, seq_with_oov):\n",
    "    print(f\"{sentence} --> {sequence}\")\n",
    "\n",
    "# To check:\n",
    "vocab_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really love my dog --> tensor([6., 1., 2., 3., 5.])\n",
      "my dog love my manatee --> tensor([3., 5., 2., 3., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'?': 10,\n",
       " '!': 8,\n",
       " '.': 9,\n",
       " 'i': 6,\n",
       " 'dog': 5,\n",
       " 'do': 11,\n",
       " 'cat': 4,\n",
       " 'my': 3,\n",
       " 'you': 7,\n",
       " 'love': 2,\n",
       " '<unk>': 1,\n",
       " '<pad>': 0}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "sentences_oov = [\n",
    "    \"i really love my dog\",\n",
    "    \"my dog love my manatee\"\n",
    "]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_token(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "# Build vocab using old sentences to see how the OOV works on new sentences later\n",
    "vocab = build_vocab_from_iterator(yield_token(sentences), specials=['<pad>', '<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Numericalized\n",
    "seq_list = []\n",
    "for sentence in sentences_oov:\n",
    "    tokens = tokenizer(sentence)\n",
    "    seq = vocab(tokens)\n",
    "    seq_list.append(torch.Tensor(seq))\n",
    "\n",
    "seq_list\n",
    "\n",
    "# To get it into TF like format above:\n",
    "for sentence, sequence in zip(sentences_oov, seq_list):\n",
    "    print(f\"{sentence} --> {sequence}\")\n",
    "\n",
    "# To check\n",
    "vocab.get_itos()\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes another introduction to text data preprocessing. So far, you've just been using dummy data. In the next exercise, you will be applying the same concepts to a real-world and much larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2llm_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
