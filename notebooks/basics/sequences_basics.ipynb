{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting input sentences into numeric sequences. Similar to images in the previous course, you need to prepare text data with uniform size before feeding it to your model. You will see how to do these in the next sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab, you saw how to use the TextVectorization layer to build a vocabulary from your corpus. It generates a list where more frequent words have lower indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 14:42:50.162812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-24 14:42:51.139928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '[UNK]', 'my', 'love', 'you', 'i', 'dog', 'cat', 'do']\n",
      "\n",
      "with indices:\n",
      "\n",
      "0 \n",
      "1 [UNK]\n",
      "2 my\n",
      "3 love\n",
      "4 you\n",
      "5 i\n",
      "6 dog\n",
      "7 cat\n",
      "8 do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 14:42:52.949187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-24 14:42:52.972362: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog.\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "    \"Do you love my cat?\"\n",
    "]\n",
    "\n",
    "# Initialize the layer\n",
    "vec_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Compute the vocab\n",
    "vec_layer.adapt(sentences)\n",
    "\n",
    "# get the vocab\n",
    "vocab_tf = vec_layer.get_vocabulary()\n",
    "\n",
    "print(f\"Vocabulary: {vocab_tf}\")\n",
    "print(\"\\nwith indices:\\n\")\n",
    "for index, word in enumerate(vocab_tf):\n",
    "    print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['<pad>', '<unk>', 'love', 'my', 'cat', 'dog', 'i', 'you', '!', '.', '?', 'do']\n",
      "\n",
      "With Indices:\n",
      "\n",
      "{'?': 10, '!': 8, '.': 9, 'i': 6, 'dog': 5, 'do': 11, 'cat': 4, 'my': 3, 'you': 7, 'love': 2, '<unk>': 1, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog.\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "    \"Do you love my cat?\"\n",
    "]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_token(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_token(sentences), specials=['<pad>', '<unk>'])\n",
    "\n",
    "print(f\"Vocabulary: {vocab.get_itos()}\")\n",
    "print(\"\\nWith Indices:\\n\")\n",
    "print(vocab.get_stoi())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the result to convert each of the input sentences into integer sequences. See how that's done below given a single input string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5 3 2 6], shape=(4,), dtype=int64)\n",
      "vocab: ['', '[UNK]', 'my', 'love', 'you', 'i', 'dog', 'cat', 'do']\n"
     ]
    }
   ],
   "source": [
    "# string input\n",
    "sample_input = \"I love my dog\"\n",
    "\n",
    "# convert string input to integer sequence\n",
    "seq = vec_layer(sample_input)\n",
    "\n",
    "print(seq)\n",
    "\n",
    "# To Check\n",
    "print(f\"vocab: {vocab_tf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2, 3, 5]\n",
      "{'?': 10, '!': 8, '.': 9, 'i': 6, 'dog': 5, 'do': 11, 'cat': 4, 'my': 3, 'you': 7, 'love': 2, '<unk>': 1, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "# string input\n",
    "sample_input = \"I love my dog\"\n",
    "\n",
    "# convert string input to integer sequence\n",
    "tokens = tokenizer(sample_input)\n",
    "\n",
    "# get the tokens\n",
    "seq = vocab(tokens)\n",
    "\n",
    "print(seq)\n",
    "\n",
    "# to check\n",
    "print(vocab.get_stoi())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, you simply pass in the string to the layer which already learned the vocabulary, and it will output the integer sequence as a tf.Tensor. In this case, the result is [6 3 2 4]. You can look at the token index printed above to verify that it matches the indices for each word in the input string.\n",
    "\n",
    "For a given list of string inputs (such as the 4-item sentences list above), you will need to apply the layer to each input. There's more than one way to do this. Let's first use the map() method and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "I love my dog. --> [5 3 2 6]\n",
      "I love my cat --> [5 3 2 7]\n",
      "You love my dog! --> [4 3 2 6]\n",
      "Do you love my cat? --> [8 4 3 2 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 14:42:55.523119: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [4]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)\n",
    "\n",
    "\n",
    "# convert sentences to tf data\n",
    "sentences_dataset = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "\n",
    "# define a mapping function to convert each sample input\n",
    "sequences = sentences_dataset.map(vec_layer)\n",
    "\n",
    "# print integer sequences\n",
    "\n",
    "for sentence, sequence in zip(sentences, sequences):\n",
    "    print(f\"{sentence} --> {sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "I love my dog. --> [6, 2, 3, 5, 9]\n",
      "I love my cat --> [6, 2, 3, 4]\n",
      "You love my dog! --> [7, 2, 3, 5, 8]\n",
      "Do you love my cat? --> [11, 7, 2, 3, 4, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'?': 10,\n",
       " '!': 8,\n",
       " '.': 9,\n",
       " 'i': 6,\n",
       " 'dog': 5,\n",
       " 'do': 11,\n",
       " 'cat': 4,\n",
       " 'my': 3,\n",
       " 'you': 7,\n",
       " 'love': 2,\n",
       " '<unk>': 1,\n",
       " '<pad>': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences)\n",
    "\n",
    "# get tokens for sentences\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    # print(tokens)\n",
    "    seq = vocab(tokens)\n",
    "    print(f\"{sentence} --> {seq}\")\n",
    "\n",
    "# To check\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of varying lengths to have a uniform size by padding or truncating tokens from the sequences. Padding is more common to preserve information.\n",
    "\n",
    "Recall that your vocabulary reserves a special token index 0 for padding. It will add that token (called post padding) if you pass in a list of string inputs to the layer. See an example below. Notice that you have the same output as above but the integer sequences are already post-padded with 0 up to the length of the longest sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "INPUT:\n",
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "\n",
      "OUTPUT:\n",
      "tf.Tensor(\n",
      "[[5 3 2 6 0]\n",
      " [5 3 2 7 0]\n",
      " [4 3 2 6 0]\n",
      " [8 4 3 2 7]], shape=(4, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(sentences)\n",
    "\n",
    "# apply the layer to the input list\n",
    "seq_post = vec_layer(sentences)\n",
    "print('INPUT:')\n",
    "print(sentences)\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(seq_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want pre-padding, you can use the pad_sequences() utility to prepend a padding token to the sequences. Notice that the padding argument is set to pre. This is just for clarity. The function already has this set as the default so you can opt to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None)>\n",
      "INPUT:\n",
      "[5 3 2 6]\n",
      "[5 3 2 7]\n",
      "[4 3 2 6]\n",
      "[8 4 3 2 7]\n",
      "\n",
      "OUTPUT:\n",
      "[[0 5 3 2 6]\n",
      " [0 5 3 2 7]\n",
      " [0 4 3 2 6]\n",
      " [8 4 3 2 7]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "\n",
    "seq_pre = tf.keras.utils.pad_sequences(sequences, padding='pre')\n",
    "\n",
    "# # For Post\n",
    "# seq_post = tf.keras.utils.pad_sequences(sequences, padding='post')\n",
    "# seq_post = tf.keras.utils.pad_sequences(sequences)\n",
    "\n",
    "\n",
    "print('INPUT:')\n",
    "[print(sequence.numpy()) for sequence in sequences]\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(seq_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "[5 3 2 6]\n",
      "[5 3 2 7]\n",
      "[4 3 2 6]\n",
      "[8 4 3 2 7]\n",
      "\n",
      "OUTPUT:\n",
      "[[5 3 2 6]\n",
      " [5 3 2 7]\n",
      " [4 3 2 6]\n",
      " [4 3 2 7]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "\n",
    "seq_pre = tf.keras.utils.pad_sequences(sequences, maxlen=4, padding='pre')\n",
    "\n",
    "print('INPUT:')\n",
    "[print(sequence.numpy()) for sequence in sequences]\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(seq_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None)>\n",
      "INPUT:\n",
      "[5 3 2 6]\n",
      "[5 3 2 7]\n",
      "[4 3 2 6]\n",
      "[8 4 3 2 7]\n",
      "\n",
      "OUTPUT:\n",
      "[[5 3 2 6]\n",
      " [5 3 2 7]\n",
      " [4 3 2 6]\n",
      " [8 4 3 2]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "\n",
    "seq_post = tf.keras.utils.pad_sequences(sequences, maxlen=4, truncating='post', padding='pre')\n",
    "\n",
    "print('INPUT:')\n",
    "[print(sequence.numpy()) for sequence in sequences]\n",
    "print()\n",
    "\n",
    "print('OUTPUT:')\n",
    "print(seq_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love my dog.', 'I love my cat', 'You love my dog!', 'Do you love my cat?']\n",
      "tensor_list: \n",
      "\n",
      "[tensor([ 4,  2,  3, 10]), tensor([4, 2, 3, 6]), tensor([5, 2, 3, 9]), tensor([8, 5, 2, 3, 7])]\n",
      "tensor([[ 4,  2,  3, 10,  0],\n",
      "        [ 4,  2,  3,  6,  0],\n",
      "        [ 5,  2,  3,  9,  0],\n",
      "        [ 8,  5,  2,  3,  7]])\n",
      "\n",
      "{'dog.': 10, 'dog!': 9, 'cat?': 7, 'do': 8, 'cat': 6, 'i': 4, 'my': 3, 'you': 5, 'love': 2, '<unk>': 1, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "print(sentences)\n",
    "\n",
    "tokenizer = lambda x: x.lower().split()\n",
    "\n",
    "def yield_token(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_token(sentences), specials=['<pad>', '<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "#numericalized\n",
    "list_num = []\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    seq = vocab(tokens)\n",
    "    list_num.append(torch.tensor(seq))\n",
    " \n",
    "print(\"tensor_list: \\n\")\n",
    "print(list_num)\n",
    "\n",
    "padding = pad_sequence(sequences=list_num, batch_first=True, padding_value=vocab['<pad>'])\n",
    "print(padding)\n",
    "print()\n",
    "print(vocab.get_stoi())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch build in only supports post-padding. To pre-pad define function manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  2,  3, 10],\n",
       "        [ 0,  4,  2,  3,  6],\n",
       "        [ 0,  5,  2,  3,  9],\n",
       "        [ 8,  5,  2,  3,  7]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def pre_pad(sequences, padding_value, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        pad_len = max_len - len(seq)\n",
    "        padded_seq = torch.cat([torch.full((pad_len,), padding_value), seq])\n",
    "        padded.append(padded_seq)\n",
    "    return torch.stack(padded)\n",
    "\n",
    "\n",
    "pad_pre = pre_pad(list_num, padding_value=vocab['<pad>'])\n",
    "pad_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncating is also not supported in PyTorch built-in. So we will have to define it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 4,  2,  3, 10]), tensor([4, 2, 3, 6]), tensor([5, 2, 3, 9]), tensor([8, 5, 2, 3, 7])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing_extensions import Literal\n",
    "literal_ = Literal['pre', 'post']\n",
    "print(list_num)\n",
    "\n",
    "def truncate_torch(sequences: list[torch.Tensor], len_: int, type: literal_):\n",
    "\n",
    "    list_len = [len(seq) for seq in sequences]\n",
    "    max_len = max(list_len)\n",
    "    print(max_len)\n",
    "    min_len = min(list_len)\n",
    "    new_seq_list = []\n",
    "    if len_ < max_len:\n",
    "        for seq in sequences:\n",
    "            len_to_del = len(seq) - len_\n",
    "            if type == 'pre':\n",
    "                # indices_to_del = torch.Tensor([i for i in range(0, len_to_del)])\n",
    "                new_seq = seq[len_to_del:]\n",
    "            elif type == 'post':\n",
    "                # indices_to_del = torch.Tensor([i for i in range(-1, -len_to_del-1, -1)])\n",
    "                new_seq = seq[0:-len_to_del]\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Please choose type from {literal_}\")\n",
    "            new_seq_list.append(new_seq)\n",
    "            print(new_seq_list)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Nothing to truncate. Given length to truncate - {len_} > maximum length of sequences - {max_len}\")\n",
    "    \n",
    "    return new_seq_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(-1, -2 -1, -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1, 2, 3,4, 5, 6, 7, 8, 9, 10])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5., 6., 7.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "truncate_torch(list_num, 2, 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list_num[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'pop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'pop'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2llm_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
