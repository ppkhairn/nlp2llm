{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most natural language processing (NLP) tasks, the initial step in preparing your data is to extract a vocabulary of words from your corpus (i.e. input texts). You will need to define how to represent the texts into numeric features which can be used to train a neural network. Tensorflow and Keras makes it easy to generate these using its APIs. You will see how to do that in the next cells.\n",
    "\n",
    "The code below takes a list of sentences, then takes each word in those sentences and assigns it to an integer. This is done using the TextVectorization() preprocessing layer and its adapt() method.\n",
    "\n",
    "As mentioned in the docs above, this layer does several things including:\n",
    "\n",
    "    1. Standardizing each example. The default behavior is to lowercase and strip punctuation. See its standardize argument for other options.\n",
    "    2. Splitting each example into substrings. By default, it will split into words. See its split argument for other options.\n",
    "    3. Recombining substrings into tokens. See its ngrams argument for reference.\n",
    "    4. Indexing tokens.\n",
    "    5. Transforming each example using this index, either into a vector of ints or a dense float vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 14:48:48.772503: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-22 14:48:50.406432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-22 14:48:54.302638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-22 14:48:54.308676: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# sample inputs\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog\",\n",
    "    \"i love my cat\"\n",
    "    ]\n",
    "\n",
    "# Initialize the layeer\n",
    "vectorized_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Build the vocab\n",
    "vectorized_layer.adapt(sentences)\n",
    "\n",
    "# get vocab\n",
    "vocab = vectorized_layer.get_vocabulary(include_special_tokens = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The resulting vocabulary will be a list where more frequently used words will have a lower index. By default, it will also reserve indices for special tokens but , for clarity, let's reserve that for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 my\n",
      "1 love\n",
      "2 i\n",
      "3 dog\n",
      "4 cat\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(vocab):\n",
    "    print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'my', 'cat', 'dog']\n",
      "{'dog': 4, 'cat': 3, 'my': 2, 'love': 1, 'i': 0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog\",\n",
    "    \"i love my cat\"\n",
    "]\n",
    "\n",
    "# whitespace tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# define yield\n",
    "def yield_token(sentences):\n",
    "    for sentenece in sentences:\n",
    "        yield tokenizer(sentenece)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_token(sentences), specials=[])\n",
    "vocab_list = vocab.get_itos() # index to string\n",
    "vocab_list_2 = vocab.get_stoi() # string to index\n",
    "print(vocab_list)\n",
    "print(vocab_list_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(vocab_list[1])\n",
    "print(vocab[\"love\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add another word. If you add another sentence, you'll notice new words in the vocabulary and new punctuation is still ignored as expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# sample inputs\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog\",\n",
    "    \"i love my cat\",\n",
    "    \"You love my dog!\"]\n",
    "\n",
    "# Initialize the layeer\n",
    "vectorized_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Build the vocab\n",
    "vectorized_layer.adapt(sentences)\n",
    "\n",
    "# get vocab\n",
    "vocab = vectorized_layer.get_vocabulary(include_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 my\n",
      "1 love\n",
      "2 i\n",
      "3 dog\n",
      "4 you\n",
      "5 cat\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(vocab):\n",
    "    print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dog', 'i', '!', 'cat', 'you']\n",
      "{'cat': 5, '!': 4, 'i': 3, 'dog': 2, 'my': 1, 'you': 6, 'love': 0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog\",\n",
    "    \"i love my cat\",\n",
    "    \"You love my dog!\"]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_token(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_token(sentences), specials=[])\n",
    "\n",
    "vocab_list_1 = vocab.get_itos()\n",
    "vocab_list_2 = vocab.get_stoi()\n",
    "\n",
    "print(vocab_list_1)\n",
    "print(vocab_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that you see how it behaves, let's include the two special tokens. The first one at 0 is used for padding and 1 is used for out-of-vocabulary words. These are important when you use the layer to convert input texts to integer sequences. You'll see that in the next lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# sample inputs\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog\",\n",
    "    \"i love my cat\",\n",
    "    \"You love my dog!\"]\n",
    "\n",
    "# Initialize the layeer\n",
    "vectorized_layer = tf.keras.layers.TextVectorization()\n",
    "\n",
    "# Build the vocab\n",
    "vectorized_layer.adapt(sentences)\n",
    "\n",
    "# get vocab\n",
    "vocab = vectorized_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 [UNK]\n",
      "2 my\n",
      "3 love\n",
      "4 i\n",
      "5 dog\n",
      "6 you\n",
      "7 cat\n"
     ]
    }
   ],
   "source": [
    "for index, words in enumerate(vocab):\n",
    "    print(index, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', 'love', 'my', 'dog', 'i', '!', 'cat', 'you']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"I love my dog\",\n",
    "    \"i love my cat\",\n",
    "    \"You love my dog!\"]\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_token(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "\"\"\"\n",
    "Add <pad> and <unk> as specials; order matters: pad first (idx 0), unk second (idx 1)\n",
    "Usually padding token is assigned index 0 because many PyTorch functions (like nn.Embedding) expect padding_idx=0.\n",
    "\"\"\"\n",
    "\n",
    "specials = ['<pad>', '<unk>']\n",
    "vocab = build_vocab_from_iterator(yield_token(sentences), specials=specials)  #or specials=['<UNK>']\n",
    "\n",
    "vocab_list_1 = vocab.get_itos()\n",
    "# vocab_list_2 - vocab.get_stoi()\n",
    "\n",
    "print(vocab_list_1)\n",
    "# print(vocab_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion on vocab building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2llm_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
